{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import functools\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.sparse import dok_matrix\n",
    "import random\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sequence Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.utils import normalized_adjacency\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "import random\n",
    "import gridfs\n",
    "import math\n",
    "import functools\n",
    "\n",
    "    \n",
    "class TwitterDataset(Sequence):\n",
    "    \n",
    "    def __init__(self, user_id, users,\n",
    "                 replies, mentions, retweets, full_graph, graph_test,\n",
    "                 max_tweets, batch_size, date_limit, db, filename='user_tweets.np'):\n",
    "        self.users_id = user_id\n",
    "        self.id_users = [None] * len(self.users_id)\n",
    "        for u_id, idx in self.users_id.items():\n",
    "            self.id_users[idx] = u_id\n",
    "        self.graph_replies = replies\n",
    "        self.graph_mentions = mentions\n",
    "        self.graph_retweets = retweets\n",
    "        self.graph_full = full_graph\n",
    "        self.max_tweets = max_tweets\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_users = list()\n",
    "        self.target_users = list(user_id.keys())\n",
    "        self.target_users.sort()\n",
    "        for u in self.target_users:\n",
    "            if u not in graph_test.nodes:\n",
    "                continue\n",
    "            if len(list(graph_test.neighbors(u))) > 0:\n",
    "                self.valid_users.append(u)\n",
    "        self.valid_users.sort()\n",
    "        #empty tweet representation\n",
    "        #bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        #tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "        #self.empty_tweet =  bert_model(**tokenizer('', return_tensors='tf'))['pooler_output'].numpy()\n",
    "        self.empty_tweet = None\n",
    "        self.date_limit = date_limit\n",
    "        self.gridfs = gridfs.GridFS(db, collection='fsProcessedTweets')\n",
    "        #del bert_model\n",
    "        #del tokenizer\n",
    "        self.filename = filename\n",
    "        self._init_tweet_cache()\n",
    "        self.current_target = -1\n",
    "        self.batch_per_pass = math.ceil(len(self.target_users)/ self.batch_size)\n",
    "        pass\n",
    "        \n",
    "    def create_data(self):\n",
    "        self.user_data = []\n",
    "        print('Preprocessing batchs')\n",
    "        for i in tqdm(range(0, len(self.valid_users))):\n",
    "            self.user_data.append(self._get_instance(self.valid_users[i]))\n",
    "            #data = [self._get_instance(self.valid_users[i])]\n",
    "            #max_users = max([len(instance[0]) for instance in data]) \n",
    "            #self.user_data.append(self._to_batch(data, max_users))\n",
    "        self.internal_get_item = self.internal_get_item_cache\n",
    "        pass\n",
    "               \n",
    "    def _init_tweet_cache(self):\n",
    "        if not os.path.exists('training_tweets.npy'):\n",
    "            self.tweets = np.zeros((len(self.id_users), 768), dtype=np.float32)\n",
    "            for i, t in tqdm(enumerate(self.id_users), total=len(self.id_users)):\n",
    "                self.tweets[i, ...] = self._get_tweets_bert_base(t)\n",
    "            np.save('training_tweets.npy', self.tweets)\n",
    "            return\n",
    "        self.tweets = np.load('training_tweets.npy')\n",
    "        self.tweets = np.mean(self.tweets, axis=1)\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.valid_users) *  self.batch_per_pass\n",
    "    \n",
    "    def _get_graph_for_node(self, node):\n",
    "        user = node#self.user_id[node]\n",
    "        node_map = {user: 0}\n",
    "        #Maps all the 1-level node to create the matrix\n",
    "        for neighbor in self.graph_replies.neighbors(node):\n",
    "            if neighbor not in node_map:\n",
    "                node_map[neighbor] = len(node_map)\n",
    "        for neighbor in self.graph_mentions.neighbors(node):\n",
    "            if neighbor not in node_map:\n",
    "                node_map[neighbor] = len(node_map)\n",
    "        for neighbor in self.graph_retweets.neighbors(node):\n",
    "            if neighbor not in node_map:\n",
    "                node_map[neighbor] = len(node_map)\n",
    "        #Creates the 3 matrixes\n",
    "        replies = np.eye(len(node_map))\n",
    "        mentions = np.eye(len(node_map))\n",
    "        retweets = np.eye(len(node_map))\n",
    "        #creates the Â matrix for the key node \n",
    "        for node, node_id in node_map.items():\n",
    "            for neighbor in self.graph_replies.neighbors(node):\n",
    "                if neighbor in node_map:\n",
    "                    replies[node_id, node_map[neighbor]] = 1\n",
    "                    \n",
    "            for neighbor in self.graph_mentions.neighbors(node):\n",
    "                if neighbor in node_map:\n",
    "                    mentions[node_id, node_map[neighbor]] = 1\n",
    "                    \n",
    "            for neighbor in self.graph_retweets.neighbors(node):\n",
    "                if neighbor in node_map:\n",
    "                    retweets[node_id, node_map[neighbor]] = 1\n",
    "        replies = normalized_adjacency(replies)\n",
    "        mentions = normalized_adjacency(mentions)\n",
    "        retweets = normalized_adjacency(retweets)\n",
    "        #Create the embedding vector\n",
    "        embeddings = np.zeros((len(node_map)))\n",
    "        for k, v in node_map.items():\n",
    "            #Convert the tweeter user id to the id acording to the nn\n",
    "            embeddings[v] = self.users_id[k] \n",
    "        return embeddings, replies, mentions, retweets\n",
    "    \n",
    "    def _get_tweets_bert(self, node):\n",
    "        idx = int(node)\n",
    "        return self.tweets[idx, ...]\n",
    "\n",
    "    def _get_tweets_bert_db(self, node):\n",
    "        user_id = node\n",
    "        query = {'userId': int(user_id)}\n",
    "        if self.date_limit is not None:\n",
    "            query['created'] = {'$lte': self.date_limit}\n",
    "        cursor = (\n",
    "            self.gridfs.\n",
    "            find(query).\n",
    "            sort([('created', pymongo.DESCENDING)]).\n",
    "            limit(self.max_tweets)\n",
    "        )\n",
    "        result = np.empty((self.max_tweets, 768))\n",
    "        i = 0\n",
    "        for file in cursor:\n",
    "            result[i, :] = np.load(file)['pooler_output']\n",
    "            i += 1\n",
    "        while i < self.max_tweets:\n",
    "            result[i, :] = self.empty_tweet\n",
    "            i += 1\n",
    "        return result\n",
    "    \n",
    "    def _get_instance(self, node):\n",
    "        embeddings, replies, mentions, retweets = self._get_graph_for_node(node)\n",
    "        return embeddings, replies[:1, :], mentions[:1, :], retweets[:1, :]\n",
    "    \n",
    "    def _to_batch(self, instances, max_users, batch_size):\n",
    "        user_i = np.zeros((batch_size, max_users))\n",
    "        user_replies = np.zeros((batch_size, 1, max_users))\n",
    "        user_mentions = np.zeros((batch_size, 1, max_users))\n",
    "        user_retweet = np.zeros((batch_size, 1, max_users))\n",
    "        for i, (embeddings, replies, mentions, retweets) in enumerate(instances):\n",
    "            user_i[i, :embeddings.shape[0]] = embeddings\n",
    "            user_replies[i, :replies.shape[0], :replies.shape[1]] = replies\n",
    "            user_mentions[i, :mentions.shape[0], :mentions.shape[1]] = mentions\n",
    "            user_retweet[i, :retweets.shape[0], :retweets.shape[1]] = retweets\n",
    "        return [user_i, user_replies, user_mentions, user_retweet]\n",
    "    \n",
    "    def _to_batch_single(self, instance, repeat):\n",
    "        user_i = instance[0]\n",
    "        user_replies = instance[1]\n",
    "        user_mentions = instance[2]\n",
    "        user_retweet = instance[3]\n",
    "        user_i = np.expand_dims(user_i, axis=0)\n",
    "        user_replies = np.expand_dims(user_replies, axis=0)\n",
    "        user_mentions = np.expand_dims(user_mentions, axis=0)\n",
    "        user_retweet = np.expand_dims(user_retweet, axis=0)\n",
    "        user_i = np.repeat(user_i, repeat, axis=0)\n",
    "        user_replies = np.repeat(user_replies, repeat, axis=0)\n",
    "        user_mentions = np.repeat(user_mentions, repeat, axis=0)\n",
    "        user_retweet = np.repeat(user_retweet, repeat, axis=0)\n",
    "        return [user_i, user_replies, user_mentions, user_retweet]\n",
    "    \n",
    "    def internal_get_item_cache(self, idx):\n",
    "        current_user = idx % len(self.valid_users)\n",
    "        current_target = idx // len(self.valid_users)\n",
    "        if current_target != self.current_target: \n",
    "            target_list = self.target_users[current_target * self.batch_size : \n",
    "                                            (current_target + 1) * self.batch_size]\n",
    "            target_list = [self._get_instance(idx) for idx in target_list]\n",
    "            max_user = max([len(instance[0]) for instance in target_list])\n",
    "            self.current_target_data = self._to_batch(target_list, max_user, len(target_list))\n",
    "            self.current_target = current_target\n",
    "        target_batch = self.current_target_data\n",
    "        #Busco los datos y lo hago crecer al tamaño del batch\n",
    "        user_data =  self._to_batch_single(self.user_data[current_user], target_batch[0].shape[0])\n",
    "        return user_data + target_batch\n",
    "    \n",
    "    def internal_get_item(self, idx):\n",
    "        current_user = self.valid_users[idx % len(self.valid_users)]\n",
    "        current_target = idx // len(self.valid_users)\n",
    "        if current_target != self.current_target: \n",
    "            target_list = self.target_users[current_target * self.batch_size : \n",
    "                                            (current_target + 1) * self.batch_size]\n",
    "            target_list = [self._get_instance(idx) for idx in target_list]\n",
    "            max_user = max([len(instance[0]) for instance in target_list])\n",
    "            self.current_target_data = self._to_batch(target_list, max_user, len(target_list))\n",
    "            self.current_target = current_target\n",
    "        target_batch = self.current_target_data\n",
    "        #Busco los datos y lo hago crecer al tamaño del batch\n",
    "        user_data =  self._to_batch_single(self._get_instance(current_user), target_batch[0].shape[0])\n",
    "        return user_data + target_batch\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.internal_get_item(idx)\n",
    "    \n",
    "    def gen_users_pairs(self, idx):\n",
    "        current_user = idx % len(self.valid_users)\n",
    "        current_target = idx // len(self.valid_users)\n",
    "        target_list = self.target_users[current_target * self.batch_size : \n",
    "                                        (current_target + 1) * self.batch_size]\n",
    "        current_user = self.valid_users[current_user]\n",
    "        return [(current_user, d) for d in target_list]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tweets = 15\n",
    "batch_size = 50\n",
    "with open('test_ds.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "user_id = dataset.users_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 25)\n",
      "(500, 1, 25)\n",
      "(500, 1, 25)\n",
      "(500, 1, 25)\n",
      "(500, 341)\n",
      "(500, 1, 341)\n",
      "(500, 1, 341)\n",
      "(500, 1, 341)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset[0]:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Input, Embedding, Concatenate, \\\n",
    "                TimeDistributed, Lambda, Dot, Attention, GlobalMaxPool1D, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.layers.convolutional import GCNConv\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    #recibe indices con forma 1xvaloresx3 (indices + valor)\n",
    "    #trasnforma los indices a valoresx2 y los valores valoresx1\n",
    "    v_true, dist = y_true[:, 0], y_true[:, 1]\n",
    "    return K.mean(dist * K.square(y_pred - K.log(2 * v_true) / K.log(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user_list (InputLayer)          [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "target_list (InputLayer)        [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embeddings (Embedding)     (None, None, 64)     412800      user_list[0][0]                  \n",
      "                                                                 target_list[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "replies_user (InputLayer)       [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mentions_user (InputLayer)      [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "retweets_user (InputLayer)      [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "replies_target (InputLayer)     [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mentions_target (InputLayer)    [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "retweets_target (InputLayer)    [(None, None, None)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gcn_replies_0 (GCNConv)         (None, None, 32)     2080        user_embeddings[0][0]            \n",
      "                                                                 replies_user[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "gcn_mentions_0 (GCNConv)        (None, None, 32)     2080        user_embeddings[0][0]            \n",
      "                                                                 mentions_user[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gcn_retweets_0 (GCNConv)        (None, None, 32)     2080        user_embeddings[0][0]            \n",
      "                                                                 retweets_user[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gcn_t_replies_0 (GCNConv)       (None, None, 32)     2080        user_embeddings[1][0]            \n",
      "                                                                 replies_target[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "gcn_t_mentions_0 (GCNConv)      (None, None, 32)     2080        user_embeddings[1][0]            \n",
      "                                                                 mentions_target[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "gcn_t_retweets_0 (GCNConv)      (None, None, 32)     2080        user_embeddings[1][0]            \n",
      "                                                                 retweets_target[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "user_gnc (Concatenate)          (None, None, 96)     0           gcn_replies_0[0][0]              \n",
      "                                                                 gcn_mentions_0[0][0]             \n",
      "                                                                 gcn_retweets_0[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "target_gnc (Concatenate)        (None, None, 96)     0           gcn_t_replies_0[0][0]            \n",
      "                                                                 gcn_t_mentions_0[0][0]           \n",
      "                                                                 gcn_t_retweets_0[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "user_row (Lambda)               (None, 96)           0           user_gnc[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "target_row (Lambda)             (None, 96)           0           target_gnc[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "graph_reps_concat (Concatenate) (None, 192)          0           user_row[0][0]                   \n",
      "                                                                 target_row[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "user_wide (Lambda)              (None, 64)           0           user_embeddings[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "target_wide (Lambda)            (None, 64)           0           user_embeddings[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           6176        graph_reps_concat[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "reps_concat (Concatenate)       (None, 128)          0           user_wide[0][0]                  \n",
      "                                                                 target_wide[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            129         reps_concat[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_AddV2 (TensorFlowOp [(None, 1)]          0           dense_2[0][0]                    \n",
      "                                                                 dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 431,618\n",
      "Trainable params: 431,618\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "emb_size = 64\n",
    "kernels = 32\n",
    "deep = 1\n",
    "\n",
    "embedded = Embedding(len(user_id), emb_size, name='user_embeddings')\n",
    "\n",
    "user_i = Input(shape=(None,), name='user_list', dtype=tf.int32)\n",
    "emb_user = embedded(user_i)\n",
    "\n",
    "target_i = Input(shape=(None,), name='target_list', dtype=tf.int32)\n",
    "emb_target = embedded(target_i)\n",
    "\n",
    "replies_user_i = Input(shape=(None, None), name='replies_user', dtype=tf.float32)\n",
    "mentions_user_i = Input(shape=(None, None), name='mentions_user', dtype=tf.float32)\n",
    "retweets_user_i = Input(shape=(None, None), name='retweets_user', dtype=tf.float32)\n",
    "\n",
    "replies_target_i = Input(shape=(None, None), name='replies_target', dtype=tf.float32)\n",
    "mentions_target_i = Input(shape=(None, None), name='mentions_target', dtype=tf.float32)\n",
    "retweets_target_i = Input(shape=(None, None), name='retweets_target', dtype=tf.float32)\n",
    "\n",
    "user_emb = emb_user\n",
    "target_emb = emb_target\n",
    "\n",
    "emb_rep, emb_men, emb_rt = user_emb, user_emb, user_emb\n",
    "emb_t_rep, emb_t_men, emb_t_rt = target_emb, target_emb, target_emb\n",
    "for i in range(deep):\n",
    "    emb_rep = GCNConv(kernels, name='gcn_replies_{}'.format(i))([emb_rep, replies_user_i])\n",
    "    emb_men = GCNConv(kernels, name='gcn_mentions_{}'.format(i))([emb_men, mentions_user_i])\n",
    "    emb_rt = GCNConv(kernels, name='gcn_retweets_{}'.format(i))([emb_rt, retweets_user_i])\n",
    "    \n",
    "    emb_t_rep = GCNConv(kernels, name='gcn_t_replies_{}'.format(i))([emb_t_rep, replies_target_i])\n",
    "    emb_t_men = GCNConv(kernels, name='gcn_t_mentions_{}'.format(i))([emb_t_men, mentions_target_i])\n",
    "    emb_t_rt = GCNConv(kernels, name='gcn_t_retweets_{}'.format(i))([emb_t_rt, retweets_target_i])\n",
    "    \n",
    "mat = Concatenate(name='user_gnc')([emb_rep, emb_men, emb_rt])\n",
    "mat = Lambda(lambda x: x[:, 0, :], name='user_row')(mat)\n",
    "\n",
    "mat_t = Concatenate(name='target_gnc')([emb_t_rep, emb_t_men, emb_t_rt])\n",
    "mat_t = Lambda(lambda x: x[:, 0, :], name='target_row')(mat_t)\n",
    "#Wide \n",
    "user_wide = Lambda(lambda x: x[:, 0, :], name='user_wide')(emb_user) \n",
    "target_wide = Lambda(lambda x: x[:, 0, :], name='target_wide')(emb_target) \n",
    "wide = Concatenate(name='reps_concat')([user_wide, target_wide])\n",
    "wide = Dense(1)(wide)\n",
    "#Falta unir con bert\n",
    "mat = Concatenate(name='graph_reps_concat')([mat, mat_t])\n",
    "mat = Dense(kernels)(mat)#, [0, 2, 1]\n",
    "mat = Dense(1)(mat)\n",
    "mat = mat + wide\n",
    "model = Model([user_i, replies_user_i, mentions_user_i, retweets_user_i,\n",
    "              target_i, replies_target_i, mentions_target_i, retweets_target_i], mat)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=loss, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('connected-neg-no-bert/model_rec-neg.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76dcfcc1df3b416185046f0970fb77c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=475.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "47476\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "class OffsetLimitedDs(Sequence):\n",
    "    \n",
    "    def __init__(self, ds, offset, limit):\n",
    "        self.ds = ds\n",
    "        self.offset = offset\n",
    "        self.limit = limit\n",
    "        self.len = min(self.limit, len(self.ds) - self.offset)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ds[idx + self.offset]\n",
    "    \n",
    "\n",
    "partial = 100\n",
    "with open('connected-neg-no-bert/predictions-neg.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow(['Origin', 'Destiny', 'Prediction'])\n",
    "    for offset in tqdm(range(0, len(dataset), partial)):\n",
    "        c_ds = OffsetLimitedDs(dataset, offset, partial)\n",
    "        pred = model.predict(c_ds, max_queue_size=10)\n",
    "        j = 0\n",
    "        for i in range(len(c_ds)):\n",
    "            pairs =  dataset.gen_users_pairs(offset + i)\n",
    "            for o, d in pairs:\n",
    "                csvwriter.writerow([o, d, pred[j, 0]])\n",
    "                j = j + 1\n",
    "            \n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
