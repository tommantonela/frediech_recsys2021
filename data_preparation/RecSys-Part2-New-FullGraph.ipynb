{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vertical-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "static-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db = client.obamacare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-gentleman",
   "metadata": {},
   "source": [
    "## Levantar grafos separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "persistent-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../\"\n",
    "df = pd.read_csv(path+'USER_POLARITY_BARBERA.txt',sep='\\t',header=None).dropna(axis=1)\n",
    "df.columns = ['user_id','leaning']\n",
    "df = df.set_index('user_id')\n",
    "user_leaning = df.to_dict()['leaning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appreciated-factor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_tweets_full(user_leaning):\n",
    "    user_tweets = {}\n",
    "    tweets = []\n",
    "    min_date = datetime(3000,1,1) \n",
    "    max_date = datetime(1900,1,1)\n",
    "    users_tweets = {}\n",
    "    step = 10000\n",
    "    total = db.tweetsInfo.count_documents({'userId': {'$in': list(user_leaning.keys())}})\n",
    "    for t in tqdm(db.tweetsInfo.find({'userId': {'$in': list(user_leaning.keys())}}), total=total):\n",
    "        tweets.append(t['tweetId'])\n",
    "        user_tweets.setdefault(t['userId'], list()).append(t['tweetId'])\n",
    "        created = t['created']\n",
    "        if min_date > created:\n",
    "            min_date = created\n",
    "        if max_date < created:\n",
    "            max_date = created\n",
    "    return set(user_tweets.keys()), user_tweets, tweets, min_date, max_date\n",
    "\n",
    "\n",
    "\n",
    "def get_users_tweets(users, conditions={}):\n",
    "    users_tweets = {}\n",
    "    cond = {'userId': {'$in': list(users)}}\n",
    "    cond.update(conditions)\n",
    "    total = db.tweetsInfo.count_documents(cond)\n",
    "    for t in tqdm(db.tweetsInfo.find(cond), total=total):\n",
    "        users_tweets.setdefault(t['userId'], list()).append(t['tweetId'])\n",
    "    return users_tweets\n",
    "\n",
    "\n",
    "def add_created(link, date, link_date):\n",
    "    if link not in link_date:\n",
    "        link_date[link] = date\n",
    "        return\n",
    "    if link_date[link] > date:\n",
    "        link_date[link] = date\n",
    "    pass\n",
    "\n",
    "def users_retweeted(tweet, conditions={}):\n",
    "    if 'retweetId' not in tweet:\n",
    "        return set()\n",
    "    cond = {'tweetId': tweet['retweetId']}\n",
    "    cond.update(conditions)\n",
    "    n_tweet = db.tweetsInfo.find_one(cond)\n",
    "    if n_tweet is None:\n",
    "        return set()\n",
    "    return {n_tweet['userId']}\n",
    "    \n",
    "    \n",
    "def users_replies_from_tweet(userId, tweets, recursive, user_set, conditions={}):\n",
    "    users = Counter()\n",
    "    replies_date = dict()\n",
    "    retweets_tweets = dict()\n",
    "    cond = {'userId': userId}\n",
    "    cond.update(conditions)\n",
    "    graph = {\n",
    "        'startWith': '$inReplyToStatusId',\n",
    "        'from': 'tweetsInfo',\n",
    "        'connectFromField': 'inReplyToStatusId',\n",
    "        'connectToField': 'tweetId',\n",
    "        'as': 'replies'\n",
    "    }\n",
    "    if not recursive:\n",
    "        graph['maxDepth'] = 0\n",
    "    cursor = db.tweetsInfo.aggregate([{'$match': cond},\n",
    "                                  {'$graphLookup': graph},\n",
    "                                  {'$project': {'tweetId': 1, \n",
    "                                                'created': 1, \n",
    "                                                'replies.tweetId': 1, \n",
    "                                                'replies.userId': 1}}])\n",
    "    cursor = list(cursor)\n",
    "    cursor.sort(key=lambda x: x['created'], reverse=True)\n",
    "    processed = set()\n",
    "    for x in cursor:\n",
    "        if x['tweetId'] not in tweets:\n",
    "            continue\n",
    "        userSet = set()\n",
    "        for r in x['replies']:\n",
    "            if r['userId'] not in user_set:\n",
    "                processed.add(r['tweetId'])\n",
    "                continue\n",
    "            add_created((userId, r['userId']), x['created'], replies_date)\n",
    "            if r['tweetId'] not in processed:\n",
    "                userSet.add(r['userId'])\n",
    "                #Si ya hay un link lo actualizo\n",
    "                if (userId, r['userId']) in retweets_tweets:\n",
    "                    #Verifico si es por la misma cadena de tweets\n",
    "                    base = retweets_tweets[(userId, r['userId'])]\n",
    "                    if x['tweetId'] in base:\n",
    "                        base[x['tweetId']].append(r['tweetId'])\n",
    "                    else:\n",
    "                        base[x['tweetId']] = [r['tweetId']]\n",
    "                else:\n",
    "                    #Agrego el tweet a mi lista de bases. El link no existia hasta ahora\n",
    "                    retweets_tweets[(userId, r['userId'])] = {x['tweetId']: [r['tweetId']]}\n",
    "                processed.add(r['tweetId'])\n",
    "        if userId in userSet:\n",
    "            userSet.remove(userId)\n",
    "        for u in userSet:\n",
    "            users[(userId, u)] += 1\n",
    "    if (userId, userId) in replies_date:\n",
    "        replies_date.pop((userId, userId))\n",
    "        retweets_tweets.pop((userId, userId))\n",
    "    return users, replies_date, retweets_tweets\n",
    "\n",
    "    \n",
    "def calculate_graphs(user_tweets, recursive=True, conditions={}):\n",
    "    replies = Counter()\n",
    "    mentions = Counter()\n",
    "    retweets = Counter()\n",
    "    replies_date = dict()\n",
    "    mentions_date = dict()\n",
    "    retweets_date = dict()\n",
    "    replies_tweets = dict()\n",
    "    mentions_tweets = dict()\n",
    "    retweets_tweets = dict()\n",
    "    '''tweet_user = {}\n",
    "    for u, tweets in user_tweets.items():\n",
    "        for t in tweets:\n",
    "            tweet_user[t] = u'''\n",
    "    user_set = set(user_tweets)\n",
    "    #nus = 0\n",
    "    for user, tweets in tqdm(user_tweets.items()):\n",
    "        #nus += 1\n",
    "        #if nus > 1660:\n",
    "        #    print(user, tweets)\n",
    "        #Mentions\n",
    "        cond = {'tweetId': {'$in': tweets}}\n",
    "        cond.update(conditions)\n",
    "        for tweet in db.tweetsInfo.find(cond):\n",
    "            #Mentions\n",
    "            for mention in tweet['userMentions']:\n",
    "                m_user = mention['userId']\n",
    "                if user != m_user and m_user in user_set:\n",
    "                    mentions[(user, m_user)] += 1\n",
    "                    add_created((user, m_user), tweet['created'], mentions_date)\n",
    "                    if (user, m_user) in mentions_tweets:\n",
    "                        mentions_tweets[(user, m_user)].append(tweet['tweetId'])\n",
    "                    else:\n",
    "                        mentions_tweets[(user, m_user)] = [tweet['tweetId']]\n",
    "            #Retweet\n",
    "            if 'retweetId' not in tweet:\n",
    "                continue\n",
    "            for m_user in users_retweeted(tweet, conditions):\n",
    "                if user != m_user and m_user in user_set:\n",
    "                    retweets[(user, m_user)] += 1\n",
    "                    add_created((user, m_user), tweet['created'], retweets_date)\n",
    "                    if (user, m_user) in retweets_tweets:\n",
    "                        retweets_tweets[(user, m_user)].append(tweet['tweetId'])\n",
    "                    else:\n",
    "                        retweets_tweets[(user, m_user)] = [tweet['tweetId']]\n",
    "        #Replies\n",
    "        u_replies, u_replies_date, u_replies_tweets = users_replies_from_tweet(user, set(tweets), recursive, user_set, conditions)\n",
    "        replies += u_replies\n",
    "        replies_date.update(u_replies_date)\n",
    "        replies_tweets.update(u_replies_tweets)\n",
    "    full_users = {u: True for u in user_tweets.keys()}\n",
    "    for u in (replies + mentions + retweets).keys():\n",
    "        if u[0] not in full_users:\n",
    "            full_users[u[0]] = False\n",
    "        if u[1] not in full_users:\n",
    "            full_users[u[1]] = False\n",
    "    return full_users, replies, mentions, retweets, replies_date, mentions_date, retweets_date, replies_tweets, mentions_tweets, retweets_tweets\n",
    "\n",
    "def create_simple_graph(users, links, dates, tweets):\n",
    "    graph = nx.DiGraph()\n",
    "    for u, c in users.items():\n",
    "        graph.add_node(u, central=c)\n",
    "    for (org, dest), weight in links.items():\n",
    "        graph.add_edge(org, dest, weight=weight, date=dates[(org, dest)], tweets=tweets[(org, dest)])\n",
    "    return graph\n",
    "\n",
    "def create_full_graph(users, replies, mentions, retweets,\\\n",
    "                      replies_date, mentions_date, retweets_date,\\\n",
    "                      replies_tweets, mentions_tweets, retweets_tweets):\n",
    "    graph = nx.DiGraph()\n",
    "    for u, c in users.items():\n",
    "        graph.add_node(u, central=c)\n",
    "    links = set(replies.keys()) | set(mentions.keys()) | set(retweets.keys())\n",
    "    for l in links:\n",
    "        graph.add_edge(l[0], l[1], weight=replies.get(l, 0) + mentions.get(l, 0) + retweets.get(l, 0), \\\n",
    "                       date=min([f[l] for f in [replies_date, mentions_date, retweets_date] if l in f]), \\\n",
    "                       date_replies=replies_date.get(l, None), \\\n",
    "                       date_mentions=mentions_date.get(l, None), \\\n",
    "                       date_retweets=retweets_date.get(l, None), \\\n",
    "                       tweets_replies=replies_tweets.get(l, []), \\\n",
    "                       tweets_mentions=mentions_tweets.get(l, []), \\\n",
    "                       tweets_retweets=retweets_tweets.get(l, [])\n",
    "                      )\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continent-merit",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3750cbfe2ee24ab6b7806b67d0b4fd3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7017509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd223a0c936c4d66a5647a21c257aef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5208255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaf2feab26a46f1ac09acc325c721fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ac7333968d41df8ea175aa2c10a419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1809254 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaa37923a2142aa8e8f548e03770a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_graphs_training_test(user_leaning, date=datetime(2017, 8, 30)):\n",
    "    users, user_tweets, tweets, min_date, max_date = get_users_tweets_full(user_leaning)\n",
    "    full_users = {u: True for u  in users}\n",
    "    print('Training')\n",
    "    cond = {'created': {'$gte': min_date, '$lte': date}}\n",
    "    user_tweets = get_users_tweets(users, conditions=cond)\n",
    "    _, replies, mentions, retweets, replies_date, mentions_date, retweets_date, replies_tweets, mentions_tweets, retweets_tweets = calculate_graphs(user_tweets, conditions=cond)\n",
    "    mentions_graph_train = create_simple_graph(full_users, mentions, mentions_date, mentions_tweets)\n",
    "    replies_graph_train = create_simple_graph(full_users, replies, replies_date, replies_tweets)\n",
    "    retweets_graph_train = create_simple_graph(full_users, retweets, retweets_date, retweets_tweets)\n",
    "    graph_train = create_full_graph(full_users, replies, mentions, retweets, replies_date, mentions_date, retweets_date, replies_tweets, mentions_tweets, retweets_tweets)\n",
    "    print('test')\n",
    "    cond = {'created': {'$gt': date, '$lte': max_date}}\n",
    "    user_tweets = get_users_tweets(users, conditions=cond)\n",
    "    _, replies, mentions, retweets, replies_date, mentions_date, retweets_date, replies_tweets, mentions_tweets, retweets_tweets = calculate_graphs(user_tweets, conditions=cond)\n",
    "    mentions_graph_test = create_simple_graph(full_users, mentions, mentions_date, mentions_tweets)\n",
    "    replies_graph_test = create_simple_graph(full_users, replies, replies_date, replies_tweets)\n",
    "    retweets_graph_test = create_simple_graph(full_users, retweets, retweets_date, retweets_tweets)\n",
    "    graph_test = create_full_graph(full_users, replies, mentions, retweets, replies_date, mentions_date, retweets_date, replies_tweets, mentions_tweets, retweets_tweets)\n",
    "\n",
    "    return (graph_train, mentions_graph_train, replies_graph_train, retweets_graph_train), (graph_test, mentions_graph_test, replies_graph_test, retweets_graph_test)\n",
    "\n",
    "\n",
    "(graph_train, mentions_graph_train, replies_graph_train, retweets_graph_train), \\\n",
    "(graph_test, mentions_graph_test, replies_graph_test, retweets_graph_test) = get_graphs_training_test(user_leaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "isolated-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('train')\n",
    "nx.write_gpickle(graph_train, 'train/full.gpickle')\n",
    "nx.write_gpickle(mentions_graph_train, 'train/mentions.gpickle')\n",
    "nx.write_gpickle(replies_graph_train, 'train/replies.gpickle')\n",
    "nx.write_gpickle(retweets_graph_train, 'train/retweets.gpickle')\n",
    "os.makedirs('test')\n",
    "nx.write_gpickle(graph_test, 'test/full.gpickle')\n",
    "nx.write_gpickle(mentions_graph_test, 'test/mentions.gpickle')\n",
    "nx.write_gpickle(replies_graph_test, 'test/replies.gpickle')\n",
    "nx.write_gpickle(retweets_graph_test, 'test/retweets.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "removable-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311611\n"
     ]
    }
   ],
   "source": [
    "#Train target\n",
    "user_central = {u for u in graph_train.nodes if graph_train.nodes[u]['central']}\n",
    "cant = 0\n",
    "for n in user_central:\n",
    "    neighbors = graph_train.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            cant += 1\n",
    "\n",
    "print(cant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "proof-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125218\n"
     ]
    }
   ],
   "source": [
    "#Test target\n",
    "user_central = {u for u in graph_test.nodes if graph_train.nodes[u]['central']}\n",
    "cant = 0\n",
    "for n in user_central:\n",
    "    neighbors = graph_test.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            cant += 1\n",
    "\n",
    "print(cant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "statistical-boost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nuser_central = {u for u in graph_train.nodes if graph.nodes[u]['central']}\\norg_dest = set()\\nfor n in user_central:\\n    neighbors = graph_train.neighbors(n)\\n    for neo in neighbors:\\n        if neo in user_central:\\n            org_dest.add(n)\\n\\nreachables = set()\\n\\nfor n in org_dest:\\n    neighbors = graph_train.neighbors(n)\\n    reachables.update(neighbors)\\n\\nunreachables = set(graph_train.nodes) - (org_dest | reachables)\\ngraph_train.remove_nodes_from(unreachables)\\nmentions_graph_train.remove_nodes_from(unreachables)\\nreplies_graph_train.remove_nodes_from(unreachables)\\nretweets_graph_train.remove_nodes_from(unreachables)\\ngraph_test.remove_nodes_from(unreachables)\\nmentions_graph_test.remove_nodes_from(unreachables)\\nreplies_graph_test.remove_nodes_from(unreachables)\\nretweets_graph_test.remove_nodes_from(unreachables)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "user_central = {u for u in graph_train.nodes if graph.nodes[u]['central']}\n",
    "org_dest = set()\n",
    "for n in user_central:\n",
    "    neighbors = graph_train.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            org_dest.add(n)\n",
    "\n",
    "reachables = set()\n",
    "\n",
    "for n in org_dest:\n",
    "    neighbors = graph_train.neighbors(n)\n",
    "    reachables.update(neighbors)\n",
    "\n",
    "unreachables = set(graph_train.nodes) - (org_dest | reachables)\n",
    "graph_train.remove_nodes_from(unreachables)\n",
    "mentions_graph_train.remove_nodes_from(unreachables)\n",
    "replies_graph_train.remove_nodes_from(unreachables)\n",
    "retweets_graph_train.remove_nodes_from(unreachables)\n",
    "graph_test.remove_nodes_from(unreachables)\n",
    "mentions_graph_test.remove_nodes_from(unreachables)\n",
    "replies_graph_test.remove_nodes_from(unreachables)\n",
    "retweets_graph_test.remove_nodes_from(unreachables)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "interim-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311611\n",
      "125218\n",
      "5399\n",
      "3652\n"
     ]
    }
   ],
   "source": [
    "user_central = {u for u in graph_train.nodes if graph_train.nodes[u]['central']}\n",
    "cant = 0\n",
    "for n in user_central:\n",
    "    neighbors = graph_train.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            cant += 1\n",
    "\n",
    "print(cant)\n",
    "\n",
    "cant = 0\n",
    "for n in user_central:\n",
    "    neighbors = graph_test.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            cant += 1\n",
    "\n",
    "print(cant)\n",
    "\n",
    "cant = 0\n",
    "for n in user_central:\n",
    "    neighbors = graph_train.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            cant += 1\n",
    "            break\n",
    "\n",
    "print(cant)\n",
    "\n",
    "cant = 0\n",
    "for n in user_central:\n",
    "    neighbors = graph_test.neighbors(n)\n",
    "    for neo in neighbors:\n",
    "        if neo in user_central:\n",
    "            cant += 1\n",
    "            break\n",
    "\n",
    "print(cant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-shape",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aggressive-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Dot, Reshape, Add, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import functools\n",
    "import tensorflow.keras.backend as K\n",
    "from scipy.sparse import dok_matrix\n",
    "import random\n",
    "\n",
    "\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "operational-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_central = [u for u in graph_train.nodes if graph_train.nodes[u]['central']]\n",
    "user_central.sort()\n",
    "id_users = list(graph_train.nodes)\n",
    "\n",
    "def cmp(x, y):\n",
    "    if graph_train.nodes[x]['central'] and not graph_train.nodes[y]['central']:\n",
    "        return -1\n",
    "    if not graph_train.nodes[x]['central'] and graph_train.nodes[y]['central']:\n",
    "        return 1\n",
    "    return x - y\n",
    "\n",
    "id_users.sort(key=functools.cmp_to_key(cmp))\n",
    "user_id = {u: e for e, u in enumerate(id_users)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "possible-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target = []\n",
    "x_context = []\n",
    "\n",
    "y = []\n",
    "\n",
    "for edge in graph_train.edges:\n",
    "    x_target.append(user_id[edge[0]])\n",
    "    x_context.append(user_id[edge[1]])\n",
    "    x_target.append(user_id[edge[1]])\n",
    "    x_context.append(user_id[edge[0]])\n",
    "    y.append(graph_train.edges[edge]['weight'] + 1)\n",
    "    y.append(graph_train.edges[edge]['weight'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "elder-casting",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target = np.asarray(x_target, dtype=np.float32)\n",
    "x_target = np.expand_dims(x_target, axis=-1)\n",
    "x_context = np.asarray(x_context, dtype=np.float32)\n",
    "x_context = np.expand_dims(x_context, axis=-1)\n",
    "y = np.asarray(y, dtype=np.float32)\n",
    "y = np.expand_dims(y, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "brazilian-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred, a = 3.0/4.0, X_MAX=100):\n",
    "    \"\"\"\n",
    "    This is GloVe's loss function\n",
    "    :param y_true: The actual values, in our case the 'observed' X_ij co-occurrence values\n",
    "    :param y_pred: The predicted (log-)co-occurrences from the model\n",
    "    :return: The loss associated with this batch\n",
    "    \"\"\"\n",
    "    return K.sum(K.pow(K.clip(y_true / X_MAX, 0.0, 1.0), a) * K.square(y_pred - K.log(y_true)), axis=-1)\n",
    "\n",
    "\n",
    "def glove_model(vocab_size=10, vector_dim=64):\n",
    "    \"\"\"\n",
    "    A Keras implementation of the GloVe architecture\n",
    "    :param vocab_size: The number of distinct words\n",
    "    :param vector_dim: The vector dimension of each word\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_target = Input((1,), name='central_word_id')\n",
    "    input_context = Input((1,), name='context_word_id')\n",
    "\n",
    "    central_embedding = Embedding(vocab_size+1, vector_dim, input_length=1, name='central_emb')\n",
    "    central_bias = Embedding(vocab_size+1, 1, input_length=1, name='central_bias')\n",
    "\n",
    "    context_embedding = Embedding(vocab_size, vector_dim, input_length=1, name='context_emb')\n",
    "    context_bias = Embedding(vocab_size, 1, input_length=1, name='context_bias')\n",
    "\n",
    "    vector_target = central_embedding(input_target)\n",
    "    vector_context = context_embedding(input_context)\n",
    "\n",
    "    bias_target = central_bias(input_target)\n",
    "    bias_context = context_bias(input_context)\n",
    "\n",
    "    dot_product = Dot(axes=-1)([vector_target, vector_context])\n",
    "    dot_product = Reshape((1, ))(dot_product)\n",
    "    bias_target = Reshape((1,))(bias_target)\n",
    "    bias_context = Reshape((1,))(bias_context)\n",
    "\n",
    "    prediction = Add()([dot_product, bias_target, bias_context])\n",
    "\n",
    "    model = Model(inputs=[input_target, input_context], outputs=prediction)\n",
    "    model.compile(loss=custom_loss, optimizer='adam')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def glove_model(vocab_size=10, vector_dim=64):\n",
    "    \"\"\"\n",
    "    A Keras implementation of the GloVe architecture\n",
    "    :param vocab_size: The number of distinct words\n",
    "    :param vector_dim: The vector dimension of each word\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    input_target = Input((1,), name='central_word_id')\n",
    "    input_context = Input((1,), name='context_word_id')\n",
    "\n",
    "    central_embedding = Embedding(vocab_size+1, vector_dim, input_length=1, name='central_emb')\n",
    "\n",
    "    vector_target = central_embedding(input_target)\n",
    "    #vector_target = Lambda(lambda x: K.abs(x))(vector_target)\n",
    "    vector_context = central_embedding(input_context)\n",
    "    #vector_context = Lambda(lambda x: K.abs(x))(vector_context)\n",
    "\n",
    "    prediction = Dot(axes=-1)([vector_target, vector_context])\n",
    "\n",
    "    model = Model(inputs=[input_target, input_context], outputs=prediction)\n",
    "    model.compile(loss=custom_loss, optimizer='adam')\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "comic-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import math \n",
    "\n",
    "\n",
    "class GloveNegExample(Sequence):\n",
    "    \n",
    "    def __init__(self, x_target, x_content, y, users, batch_size=128, neg_sampling=1):\n",
    "        self.x_target = x_target\n",
    "        self.x_content = x_content\n",
    "        self.y = y\n",
    "        self.users = users\n",
    "        self.batch_size = batch_size\n",
    "        self.neg_sampling = 1\n",
    "        self.idx = list(range(self.y.shape[0]))\n",
    "        random.shuffle(self.idx)\n",
    "        pass\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.idx) / self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x_content = self.x_content[(idx * self.batch_size) : ((idx + 1) * self.batch_size), ...]\n",
    "        x_target = self.x_target[(idx * self.batch_size) : ((idx + 1) * self.batch_size), ...]\n",
    "        y = self.y[(idx * self.batch_size) : ((idx + 1) * self.batch_size), ...]\n",
    "        neg_size = int(y.shape[0] * self.neg_sampling)\n",
    "        x_content_neg = np.asarray([[random.randrange(self.users)] for _ in range(neg_size)])\n",
    "        x_target_neg = np.asarray([[random.randrange(self.users)] for _ in range(neg_size)])\n",
    "        y_neg = np.asarray([[1]] * neg_size)\n",
    "        return [np.concatenate((x_content, x_content_neg), axis=0), \\\n",
    "                np.concatenate((x_target, x_target_neg), axis=0)], \\\n",
    "                np.concatenate((y, y_neg), axis=0)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.idx)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "needed-companion",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "central_word_id (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "context_word_id (InputLayer)    [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "central_emb (Embedding)         (None, 1, 10)        64510       central_word_id[0][0]            \n",
      "                                                                 context_word_id[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 1)         0           central_emb[0][0]                \n",
      "                                                                 central_emb[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 64,510\n",
      "Trainable params: 64,510\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "4869/4869 [==============================] - 25s 5ms/step - loss: 0.3912\n",
      "Epoch 2/15\n",
      "4869/4869 [==============================] - 24s 5ms/step - loss: 0.2664\n",
      "Epoch 3/15\n",
      "4869/4869 [==============================] - 23s 5ms/step - loss: 0.1907\n",
      "Epoch 4/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.1514\n",
      "Epoch 5/15\n",
      "4869/4869 [==============================] - 22s 4ms/step - loss: 0.1296\n",
      "Epoch 6/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.1171\n",
      "Epoch 7/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.1092\n",
      "Epoch 8/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.1040\n",
      "Epoch 9/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.0999\n",
      "Epoch 10/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.0968\n",
      "Epoch 11/15\n",
      "4869/4869 [==============================] - 23s 5ms/step - loss: 0.0938\n",
      "Epoch 12/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.0908\n",
      "Epoch 13/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.0878\n",
      "Epoch 14/15\n",
      "4869/4869 [==============================] - 23s 5ms/step - loss: 0.0850\n",
      "Epoch 15/15\n",
      "4869/4869 [==============================] - 22s 5ms/step - loss: 0.0822\n"
     ]
    }
   ],
   "source": [
    "model = glove_model(len(id_users), 10)\n",
    "\n",
    "if not os.path.exists('embeddedings-dates.h5'):\n",
    "    model.fit(GloveNegExample(x_context, x_target, y, len(user_id)), epochs=15) \n",
    "    model.save_weights('embeddedings-dates.h5')\n",
    "else:\n",
    "    model.load_weights('embeddedings-dates.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "identified-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = K.get_value(model.layers[2].embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-placement",
   "metadata": {},
   "source": [
    "# Commuties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "particular-circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_undirected_graph(graph):\n",
    "    g = nx.Graph()\n",
    "    for node in graph.nodes:\n",
    "        g.add_node(node)\n",
    "    added = set()\n",
    "    for l in graph.edges:\n",
    "        if l not in added:\n",
    "            g.add_edge(*l)\n",
    "            added.add(l)\n",
    "            added.add(l[::-1])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "functioning-vessel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import json\n",
    "\n",
    "if not os.path.exists('commutities_v2.json'):\n",
    "    com = list(greedy_modularity_communities(to_undirected_graph(graph_train)))\n",
    "    with open('commutities_v2.json', 'w') as outfile:\n",
    "        json.dump([list(x) for x in com], outfile)\n",
    "else:\n",
    "    with open('commutities_v2.json', 'r') as json_file:\n",
    "        com = [set(x) for x in json.load(json_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "corresponding-canberra",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_com = {}\n",
    "for c, group in enumerate(com):\n",
    "    for u in group:\n",
    "        user_com[u] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "intelligent-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cos.npz'):\n",
    "    users_embs = embeddings[0:len(user_central), :]\n",
    "\n",
    "    cos = np.dot(users_embs, users_embs.T)\n",
    "    len_emb = np.expand_dims(np.sum(users_embs ** 2, axis=1) ** 0.5, axis=-1)\n",
    "    cos = cos / np.dot(len_emb, len_emb.T)\n",
    "    np.savez_compressed('cos.npz', cosines=cos)\n",
    "else:\n",
    "    cos = np.load('cos.npz')['cosines']\n",
    "del embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "seven-school",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad2d42b0995411aa3a3dbba404a529b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import deque\n",
    "sim = deque()\n",
    "diff = deque()\n",
    "all_cos = deque()\n",
    "for i in tqdm(range(cos.shape[0])):\n",
    "    for j in range(i+1, cos.shape[1]):\n",
    "        all_cos.append(cos[i, j])\n",
    "        if user_com[id_users[i]] == user_com[id_users[j]]:\n",
    "            sim.append(cos[i, j])\n",
    "        else:\n",
    "            diff.append(cos[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "protective-humanity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20798025\n",
      "0.30522174\n",
      "0.33704013\n",
      "9623408\n",
      "0.37413183\n",
      "0.34416634\n",
      "11174617\n",
      "0.24587648\n",
      "0.3190679\n"
     ]
    }
   ],
   "source": [
    "print(len(all_cos))\n",
    "print(np.mean(all_cos))\n",
    "print(np.std(all_cos))\n",
    "print(len(sim))\n",
    "print(np.average(sim))\n",
    "print(np.std(sim))\n",
    "print(len(diff))\n",
    "print(np.average(diff))\n",
    "print(np.std(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "linear-pastor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=876.3486836895261, pvalue=0.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "ttest_ind(sim, diff, alternative='greater', equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "occupied-posting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=516.9322873042893, pvalue=0.0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(sim, all_cos, alternative='greater', equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "black-horse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=491.61336296710385, pvalue=0.0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(all_cos, diff, alternative='greater', equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "duplicate-radical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.4030889 , 0.3625932 , 0.17443882, 0.4852578 ],\n",
       "       [0.4030889 , 1.        , 0.6120374 , 0.6151954 , 0.49913073],\n",
       "       [0.3625932 , 0.6120374 , 0.9999999 , 0.55664915, 0.7531488 ],\n",
       "       [0.17443882, 0.6151954 , 0.55664915, 1.        , 0.68268776],\n",
       "       [0.4852578 , 0.49913073, 0.7531488 , 0.68268776, 1.0000001 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos[:5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "strategic-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "del sim\n",
    "del diff\n",
    "del all_cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "hairy-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(graph_train, 'graph_train.gpickle')\n",
    "nx.write_gpickle(mentions_graph_train, 'mentions_graph_train.gpickle')\n",
    "nx.write_gpickle(replies_graph_train, 'replies_graph_train.gpickle')\n",
    "nx.write_gpickle(retweets_graph_train, 'retweets_graph_train.gpickle')\n",
    "nx.write_gpickle(graph_test, 'graph_test.gpickle')\n",
    "nx.write_gpickle(mentions_graph_test, 'mentions_graph_test.gpickle')\n",
    "nx.write_gpickle(replies_graph_test, 'replies_graph_test.gpickle')\n",
    "nx.write_gpickle(retweets_graph_test, 'retweets_graph_test.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-constant",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
